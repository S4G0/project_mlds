# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16fjOwHPbjqlvnmxOmvcbkGqj2wViL-fC
"""

!pip install dvc dvc-gdrive
!apt install tree git
!pip install pyngrok
!pip install mlflow==2.1.0

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import subprocess
from IPython import get_ipython
from IPython.display import display
import dvc
from sklearn.model_selection import train_test_split
import tensorflow as tf
import mlflow

# Ignorar warnings.
import warnings
warnings.filterwarnings('ignore')

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/S4G0/project_mlds.git
# %cd project_mlds/
!ls
!git config --global user.email "sagomezar@unal.edu.co"
!git config --global user.name "S4G0"

from google.colab import drive
drive.mount('/content/drive')
path="/content/drive/MyDrive/Proyecto Metodologías ágiles para el desarrollo de aplicaciones con ML/Data/"

#ID del drive donde están los datos versionados
drive_id = "14xKJMlBPaIOgNHjV1RkJsqFBxjnPr69L" # reemplace aquí el id de su carpeta
os.environ["DRIVEID"] = drive_id

#Se importa el archivo credentials.json del repositorio
import json
with open(path+"credentials.json") as f:
    os.environ["GDRIVE_CREDENTIALS_DATA"] = f.read()

!cp -r /content/drive/MyDrive/Proyecto\ Metodologías\ ágiles\ para\ el\ desarrollo\ de\ aplicaciones\ con\ ML/Data/Imágenes/ /content/project_mlds/data

!git status --porcelain

!dvc status --quiet

# Se traen los datos
#!dvc pull

"""##**MLFLOW**"""

command = """
mlflow server \
        --backend-store-uri sqlite:///tracking.db \
        --default-artifact-root file:mlruns \
        -p 5000 &
"""
get_ipython().system_raw(command)

"""Ahora debe agregar su token de `ngrok`:"""

token = "2Y3EvQj4U0dEbOuzgzpllybfnub_248exjXrHv2FsqRTozFmG" # Agregue el token dentro de las comillas
os.environ["NGROK_TOKEN"] = token

"""Nos autenticamos en ngrok:"""

!ngrok authtoken $NGROK_TOKEN

"""Ahora, lanzamos la conexión con ngrok:"""

from pyngrok import ngrok
ngrok.connect(5000, "http")

"""Especificamos que MLFlow debe usar el servidor que estamos manejando."""

mlflow.set_tracking_uri("http://localhost:5000")

"""Creamos un experimento:"""

exp = mlflow.create_experiment(name="Melanoma_analysis", artifact_location="mlruns")

"""# **1. Particion del conjunto de datos**
---
"""

list_name_original_benign=[]
list_name_original_malignant=[]
list_name_test_benign=[]
list_name_test_malignant=[]

for dirname, _, filenames in os.walk('./data/Imágenes/train/malignant'):
    for filename in filenames:
        list_name_original_malignant.append(os.path.join(dirname, filename))

for dirname, _, filenames in os.walk('./data/Imágenes/train/benign'):
    for filename in filenames:
        list_name_original_benign.append(os.path.join(dirname, filename))

for dirname, _, filenames in os.walk('./data/Imágenes/test/malignant'):
    for filename in filenames:
        list_name_test_malignant.append(os.path.join(dirname, filename))

for dirname, _, filenames in os.walk('./data/Imágenes/test/benign'):
    for filename in filenames:
        list_name_test_benign.append(os.path.join(dirname, filename))

df_original_benign = pd.DataFrame({'filename': list_name_original_benign})
df_original_benign['label'] = "0"

df_test_benign = pd.DataFrame({'filename': list_name_test_benign})
df_test_benign['label'] = "0"

#--------------------------------------------------------------------
df_original_malignant = pd.DataFrame({'filename': list_name_original_malignant})
df_original_malignant['label'] = "1"

df_test_malignant = pd.DataFrame({'filename': list_name_test_malignant})
df_test_malignant['label'] = "1"

df_train_benign, df_validation_benign = train_test_split(df_original_benign, test_size=0.2, random_state=0)
df_train_malignant, df_validation_malignant = train_test_split(df_original_malignant, test_size=0.2, random_state=0)

df_train = pd.concat([df_train_benign,      df_train_malignant])
df_val   = pd.concat([df_validation_benign, df_validation_malignant])
df_test  = pd.concat([df_test_benign,       df_test_malignant])

"""# **2. Selección y diseño de modelos**
---

## 2.1. Data Augmentation
"""

# Definición de función de data augmentation

# Definimos el generador de datos para el conjunto de entrenamiento
train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
        rescale=1./255,
        width_shift_range=0.0,
        height_shift_range=0.0,
        zoom_range=0,
        horizontal_flip=True,
        vertical_flip=True,
        fill_mode= 'constant',
        rotation_range= 45
    )

# Definimos el generador de datos para el conjunto de validación
val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)

# Definimos el generador de datos para el conjunto de prueba
test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)

batch_size = 128

train_gen = train_datagen.flow_from_dataframe(
    dataframe=df_train,
    x_col="filename",
    y_col="label",  # Assuming 'label' is the column containing the labels
    target_size=(300,300),
    batch_size=batch_size,
    class_mode="binary",  # It can be categorical
    seed=0
)

# Set up the generator for validation data
validation_gen = val_datagen.flow_from_dataframe(
    dataframe=df_val,
    x_col="filename",
    y_col="label",  # Assuming 'label' is the column containing the labels
    target_size=(300,300),
    batch_size=batch_size,
    class_mode="binary",  # It can be categorical
    seed=0
)

test_gen = test_datagen.flow_from_dataframe(
    dataframe=df_test,
    x_col="filename",
    y_col="label",  # Assuming 'label' is the column containing the labels
    target_size=(300,300),
    batch_size=batch_size,
    class_mode="binary",  # It can be categorical
    seed=0
)

"""## 2.2. Model 1: Red neuronal desde 0

### 2.2.1. Definición del modelo
"""

def custom_model(units, dropout):
    # Fijamos una semilla para efectos de reproducibilidad
    np.random.seed(0)
    tf.random.set_seed(0)

    # Definimos una arquitectura secuencial para la CNN
    model = tf.keras.Sequential()
    # Capa convolucional con 32 filtros y un kernel 3x3, ativación ReLU
    model.add(tf.keras.layers.Conv2D(32, (4, 4), strides=(2, 2),
    padding="valid", activation='relu', input_shape=(300, 300, 3)))
    # Capa de max pooling
    model.add(tf.keras.layers.AveragePooling2D((2, 2)))
    # Agregamos dropout para regularización
    model.add(tf.keras.layers.Dropout(dropout))
    # Capa convolucional con 64 filtros y un kernel 3x3,activación ReLU
    model.add(tf.keras.layers.Conv2D(64, (4, 4), strides=(2, 2),
    padding="valid", activation='relu'))
    # Capa de max pooling
    model.add(tf.keras.layers.AveragePooling2D((2, 2)))
    # Capa convolucional con 64 filtros y un kernel 3x3,activación ReLU
    model.add(tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1),
    padding="valid", activation='relu'))
    # Aplana la salida para la capa densa
    model.add(tf.keras.layers.Flatten())
    # Agregamos una capa densa
    model.add(tf.keras.layers.Dense(units, activation='relu'))
    # Agregamos dropout para regularización
    model.add(tf.keras.layers.Dropout(dropout))
    # agrega una capa de salida
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
    return model

dropout=0.2
neurons_in_dense_layer=32
model_test = custom_model(units=neurons_in_dense_layer, dropout=dropout)
model_test.summary()

"""### 2.2.2. Compilar el modelo"""

def compile_model(model, l_r, metrics):
    # Fijamos una semilla para efectos de reproducibiidad
    np.random.seed(0)
    tf.keras.utils.set_random_seed(0)
    # Compilamos el modelo
    model.compile(loss='binary_crossentropy',
                  optimizer=tf.keras.optimizers.Adam(learning_rate=l_r),
                  metrics=metrics)

    return model

learning_rate=1e-3

test_model = compile_model(
                          model=model_test,
                          l_r=learning_rate,
                          metrics=['accuracy']
                           )
test_model.get_compile_config()

"""## 2.2.3. Entrenar el modelo"""

def train_model(model, train_gen, val_gen, epochs, weights):
    # Fijamos una semilla para efectos de reproducibilidad
    np.random.seed(0)
    tf.random.set_seed(0)  # Usar tf.random.set_seed en lugar de tf.keras.utils.set_random_seed

    # Calcular el número de imágenes en los conjuntos de entrenamiento y validación
    num_train_images = 7684
    num_val_images = 1921

    # Definir el callback para guardar el mejor modelo
    best_callback = tf.keras.callbacks.ModelCheckpoint(filepath=weights,
                                                       monitor='val_loss',
                                                       verbose=1,
                                                       save_best_only=True,
                                                       save_weights_only=True,
                                                       mode='min')
    # Entrenar el modelo
    history = model.fit(train_gen,
                        steps_per_epoch=num_train_images//batch_size,
                        validation_data=val_gen,
                        validation_steps=num_val_images//batch_size,
                        epochs=epochs,
                        callbacks=[best_callback])

    return model, history

#Entrenamiento del modelo
epochs=30
model_tr, history = train_model(model=test_model,
                                train_gen=train_gen,
                                val_gen=validation_gen,
                                epochs=epochs,
                                weights='weights.h5')

#Información
print(history.history.keys())
print('El modelo se ha entrenado durante',len(history.history['val_accuracy']),'epochs')
if os.path.isfile('weights.h5'):
  print("Los pesos se guardaron en 'weights.h5'")

"""## 2.2.4. Evaluar el modelo"""

def evaluate_model(model, test_gen):
    # Fijamos una semilla para efectos de reproducibiidad
    np.random.seed(0)
    tf.keras.utils.set_random_seed(0)
    metrics = metrics = model.evaluate(test_gen)
    return metrics

# Evaluación del modelo
metrics = evaluate_model(test_model, test_gen)

# Resultados de la evaluación
print("Resultados de la evaluación:")
print("Loss:", metrics[0])
print("Accuracy:", metrics[1])
print(f"En este caso el {metrics[1]*100:.2f}% de las muestras fueron clasificadas correctamente por el modelo.")

run = mlflow.start_run(experiment_id = exp, run_name="Melanoma_model.v.1.0")
mlflow.sklearn.log_model(model_tr, "model")
mlflow.log_params({"dropout": dropout, "neurons_in_dense_layer": neurons_in_dense_layer, "learning_rate": learning_rate, "epochs": epochs})
mlflow.log_metrics({"Loss": metrics[0], "Accuracy": metrics[1]})

mlflow.log_artifact("weights.h5", "weights.h5")

#model=mlflow.pyfunc.load_model("models:/Melanoma/Production")

mlflow.end_run()